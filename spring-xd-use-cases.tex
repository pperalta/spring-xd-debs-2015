\section{Use Cases}
\label{sec:Use Cases}

The following customer use cases have been observed in the field by the Spring XD team.

\subsection{Fault Detection}
\textit{Challenge}: A large equipment manufacturer needed to
ingest machine data to apply predictive analytics to proactively monitor
performance and adjust business operations.

\textit{Solution}: Given the extensible architecture in Spring XD, a custom
source module was created to handle proprietary data formats, thus allowing
consumption and transformation of data to comply with their
standards. The predictive models, complying with the PMML specification, were
generated based on historical trends. Spring XD's `analytic-pmml' processor was
used in a stream to intercept machine events to compute predictions in
real-time. The outliers were captured in Redis for dashboard alerts
and ad-hoc data analysis via REST.

\subsection{Hadoop Toolset Consolidation}

\textit{Challenge}: A large retail provider is heavily invested in the Hadoop ecosystem
They wanted to avoid the development and operational complexity of using many
different tools. They wanted to streamline and modernize their Hadoop workflows.

\textit{Solution}: As one-stop runtime, Spring XD provides dozens of data integration
adapters to send and receive data from external applications, thus allowing
the customer to use the unified approach for all ingestion use-cases. This
greatly simplified the development and runtime operations for this customer
and allowed them to extract greater value from their investment in Hadoop.

\subsection{Data Ingest}
\textit{Challenge}: A startup in San Francisco was looking for a solution to
unify stream and batch operations.

\textit{Solution}: Spring XD was adopted as the standard tool for ingesting data.
A key factor in this decision was the ability of Spring XD to handle streaming
and batching as first class citizens under a single system.

\subsection{Closed-loop Analytics}
\textit{Challenge}: A finance institution wanted to build a platform to detect
fraudulent transactions. They had years of historical data in varying formats
and the transactions happening in real-time at high volume. They wanted to
automate generation of historical models from the raw dataset and apply the models
to real-time events.

\textit{Solution}: A batch job was created to learn from historical data thus
producing predictive models complying with the PMML specification. The last step
in the batch workflow delivered generated models to streams that ingest for
real-time predictions. The predictions were persisted in in-memory data grids
to feed their dashboard.
